{"name":"qats2016","tagline":"LREC 2016 Workshop on Quality Assessment for Text Simplification (QATS)","body":"## Introduction\r\n\r\nThis workshop aims to bring together researchers interested in all aspects of evaluation of automatic text simplification (ATS) systems, including automatic, human, and user-focused evaluation. It addresses the current problem in the text simplification community that there are no common standards and evaluation methodologies which would enable fair comparison of different ATS systems.\r\n\r\nGiven the close relatedness of the problem of automatic evaluation of ATS system to well-studied problems of automatic evaluation and quality estimation in machine translation (MT), the workshop also features a shared task on automatic evaluation (quality assessment) of ATS systems. We hope that this shared task will bring together the researches from MT and TS communities. We will provide training and test data-sets for English (obtained by using three different ATS approaches) and several baselines.\r\n\r\nWe solicit papers describing resources, models and techniques for evaluation of text simplification systems, proposing automatic evaluation metrics or discussing problems in user-focused evaluation, and the papers describing systems which participated in the shared task.\r\n\r\nThe workshop will feature an invited speaker, oral and poster presentations, and a closing panel discussion.\r\n\r\n\r\n## Motivation and Topics of Interest\r\n\r\nIn recent years, there has been an increasing interest in automatic text simplification (ATS) and text adaptation to various target populations. However, studies concerning evaluation of ATS systems are still very scarce and there are no methods proposed for directly comparing performances of different systems. Previous studies (Štajner et al. 2013) showed that machine translation automatic evaluation metrics such as BLEU, METEOR and TINE provide a good baseline for this task. \r\n\r\nBy organising a shared task, we wish to bring together researchers working on automatic evaluation and quality estimation of machine translation output, to participate and try to adapt their metrics to this closely related task. This would provide an opportunity to establish some metrics for automatic evaluation of ATS systems and enable their direct comparison in terms of the quality of the generated output, as well as less time consuming assessment of each ATS system. \r\n\r\nApart from the papers describing systems which participated in the shared task, we also solicit papers on all aspects of evaluation of text simplification systems (automatic, human, and user-focused) hoping to provide an opportunity for researchers to exchange their experiences and discuss common problems in all aspects of ATS evaluation.  \r\n\r\n\r\nTopics of interest include, but are not limited to:\r\n\r\n- Resources for evaluation of text simplification systems and evaluation schemes\r\n- Automatic evaluation metrics and complexity metrics\r\n- Quality assessment for text simplification\r\n- Problems in human evaluation of text simplification systems\r\n- Problems in user-focused evaluation of text simplification systems\r\n- Similarities and dissimilarities of automatic evaluation of text simplification systems with automatic evaluation of machine translation and/or summarisation systems\r\n- NLP tools for enhancing evaluation of text simplification systems\r\n- User studies\r\n\r\n\r\n## Submission information\r\n\r\nWe encourage contributions in the form of full papers (up to 8 pages long + 2 pages references) and short papers (up to 4 pages long + 2 pages references). \r\nPapers should follow the LREC format which is available at the LREC 2016 Web Site (http://lrec2016.lrec-conf.org/en/).\r\nThe submission process will be online using the START conference system.\r\n\r\nAll accepted papers will be presented orally or as a poster and published in the workshop proceedings.\r\n\r\n## Identify, describe and share your LRs!\r\n\r\nDescribing your LRs in the LRE Map is now a normal practice in the submission procedure of LREC (introduced in 2010 and adopted by other conferences).\r\n\r\nTo continue the efforts initiated at LREC 2014 about “Sharing LRs” (data, tools, web-services, etc.), authors will have the possibility, when submitting a paper, to upload LRs in a special LREC repository. This effort of sharing LRs, linked to the LRE Map for their description, may become a new “regular” feature for conferences in our field, thus contributing to creating a common repository where everyone can deposit and share data.\r\n\r\nAs scientific work requires accurate citations of referenced work so as to allow the community to understand the whole context and also replicate the experiments conducted by other researchers, LREC 2016 endorses the need to uniquely identify LRs through the use of the International Standard Language Resource Number (ISLRN, www.islrn.org), a Persistent Unique Identifier to be assigned to each Language Resource. The assignment of ISLRNs to LRs cited in LREC papers will be offered at submission time.\r\n\r\n\r\n## Important dates\r\n\r\nShared task description and release of training data: December 2015  \r\nShared task release of test set: to be announced soon  \r\nShared task results: to be announced soon  \r\nPaper submission deadline: 7th February 2016   \r\nNotification of acceptance: 8th March 2016  \r\nCamera-ready version: 25th March 2016  \r\nWorkshop: 28th May 2016  \r\n\r\n## Organisation committee\r\n\r\nSanja Štajner (University of Lisbon, Portugal)  \r\nMaja Popović (Humboldt University of Berlin, Germany)  \r\nHoracio Saggion (Universitat Pompeu Fabra, Spain)  \r\nLucia Specia (University of Sheffield, UK)  \r\nMark Fishel (University of Tartu, Estonia)  \r\n\r\n## Program committee\r\n\r\nSandra Aluisio (University of São Paolo)  \r\nEleftherios Avramidis (DFKI Berlin)  \r\nSusana Bautista (Federal University of Rio Grande do Sul)  \r\nStefan Bott (University of Stuttgart)  \r\nRichard Evans (University of Wolverhampton)  \r\nMark Fishel (University of Tartu)  \r\nSujay Kumar Jahuar (Carnegie Mellon University)  \r\nDavid Kauchak (Pomona College)  \r\nRuslan Mitkov (University of Wolverhampton)  \r\nGustavo Paetzold (University of Sheffield)  \r\nMaja Popović (Humboldt University of Berlin)  \r\nMiguel Rios (University of Leeds)  \r\nHoracio Saggion (Universidad Pompeu Fabra)  \r\nCarolina Scarton (University of Sheffield)  \r\nMatthew Shardlow (University of Manchester)  \r\nAdvaith Siddharthan (University of Aberdeen)  \r\nLucia Specia  (University of Wolverhampton)  \r\nMiloš Stanojević (University of Amsterdam)  \r\nSanja Štajner (University of Lisbon, Portugal)  \r\nIrina Temnikova (Qatar Computing Research Institute)  \r\nSowmya Vajjala (University of Tubingen)  \r\nVictoria Yaneva (University of Wolverhampton)  \r\n\r\n\r\n## Contact\r\n\r\nFor further information please contact us at:  \r\nstajner.sanja@gmail.com or maja.popovic@hu-berlin.de\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}