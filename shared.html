
<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>qats2016 by qats2016</title>
  </head>

  <body>
    <center>
    <header>
      <div class="inner">
        <h1>qats2016</h1>
        <h2>LREC 2016 Workshop & Shared Task on </h2>
        <h2>Quality Assessment for Text Simplification (QATS)</h2>  
        <h3 style="color: #A7A7A7">28th May 2016</h3>
        <h3 style="color: #A7A7A7">Portoro≈æ, Slovenia</h3>
        
      </div>
    </header>


<A HREF="index.html">home</A> 
# <A HREF="dates.html">important dates</A>
# shared task  
# <A HREF="submission.html">submission</A>
# <A HREF="proceedings.html">proceedings</A>
</center>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
         
           <h2>
 <a id="motivation" class="anchor" href="#motivation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation</h2>         
          
<p>By organising a shared task, we wish to bring together researchers working on automatic evaluation and quality 
estimation of machine translation (MT) output, to participate and try to adapt their metrics to the closely related task of automatic
text simplification (ATS). 
This would provide an opportunity to establish some metrics for automatic evaluation of ATS systems and enable 
their direct comparison in terms of the quality of the generated output, as well as less time consuming assessment 
of each ATS system. </p>
        
          <h2>
 <a id="description" class="anchor" href="#description" aria-hidden="true"><span class="octicon octicon-link"></span></a>Description</h2>         
          
<p>The shared task consists in developing a method/metric for automatic assessment of the quality of the automatically simplified English sentences. Each of the sentences has been assigned into one of the following three classes: "good", "ok" and "bad" by human evaluators. These labels will be used as reference classes.</p>

<p>Ideally, the metric should work as a classifier which automatically assigns each sentence into one of the three classes. 
Alternatively, it is also possible to submit the raw scores of the metric for each sentence.</p>
          <h3>
<a id="data" class="anchor" href="#data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data sets</h3>

<p>
The provided original sentences are from news domain and Wikipedia. 
Their corresponding automatically simplified sentences were obtained by various automatic text simplification systems and thus cover different simplification phenomena (only lexical simplification, only syntactic simplification, mixure of lexical and syntactic simplification, content reduction, etc.).
</p>
To each simplified sentence, human evaluators assigned one the three classes ("good", "ok", "bad") for each of the following four aspects:
<ol>
<li> Grammaticality (bad - ungrammatical, OK - somewhat ungrammatical but the mistakes do not impede understanding, good - completely grammatically correct)</li>
<li> Meaning preservation (bad - no meaning at all or completely opposite meaning from the original, OK - somewhat changed nuance of meaning or missing an unimportant part, good - preserved original meaning)</li>
<li> Simplicity (bad - difficult to understand, OK - somewhat difficult to understand, good - easy to understand)</li>
<li> Overall - an overall score that represents a combination of the previous three scores which penalises more meaning preservation and simplicity than grammaticality. It should help deciding whether the automatically simplified sentence is ready to be presented to a final user (good), needs post-editing (OK), or should better be discarded and simplified using some different technique or left in the original form (bad).</li>
</ol>
  
  <p><b> The training set can be downloaded here:</b> <br>
  <ul>
  <li><a href="qats2016.github.io/train.shared-task.tsv">training set</a></li>
  </ul>
  
  It consists of 505 sentence pairs and four human scores: original sentence (Original), simplified sentence (Simplified), Grammaticality score (G), Meaning preservation score (M), Simplicity score (S), and Overall score (Overall).<br>
  The Grammaticality score (G) and Simplicity score (S) are assigned only on the basis of the simplified sentence (Simplified). <br>
  The Meaning preservation score (M) and the Overall score (Overall) take into account both original sentence (Original) and its automatically simplified version (Simplified). </br></br>
   The test set (which will be released on 20th January) consists of 126 sentence pairs. The proportions of sentences from different ATS systems are the same for both training and test set. 
  </p>
  
  
   <h3>
<a id="tracks" class="anchor" href="#tracks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tracks</h3>
<p> 
There are two tracks: constrained and unconstrained. In the constrained track, you are allowed to use only the dataset provided by us (505 sentence pairs for training). In the unconstrained track, you can additionaly use any other data for training.

</p>

  

 
<h3>
<a id="submission" class="anchor" href="#submission" aria-hidden="true"><span class="octicon octicon-link"></span></a>Submission</h3>

<p>
Test set will be released on 20th January 2016.<br>
The participants should submit their metrics on 29th January 2016 in the following format:<br><br>

<pre>metric-name  sentence-number     class-value     score-value
my-metric	 1		     good	     1.78446
my-metric	 2		     ok		     5.89343
...
</pre>
</p>
<p>
  
It is possible to submit both classes and scores.<br>
Participants submitting only one of the value types should assign "x" to the whole column for the other value type.
</p>
<p>
Each participating team can send at most three different methods/metrics for each of the four aspects (G, M, S, Overall). 
    The metrics will be evaluated separately for each of the four aspects. 
    </p>
  <h3>
  <a id="eval" class="anchor" href="#eval" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluation</h3>
  <p>
    
For participants submitting classes, the metrics will be evaluated using accurracy (F-score), i.e. percentage of sentences with correctly predicted class normalised over the total number of sentences.<br>
For participants submitting raw scores, the metrics will be evaluated using Spearman's rank correlation coefficient.
  </p>
  

</body>
</html>
