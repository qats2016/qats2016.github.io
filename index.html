<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>qats2016 by qats2016</title>
  </head>

  <body>
    <center>
    <header>
      <div class="inner">
        <h1>qats2016</h1>
        <h2>LREC 2016 Workshop & Shared Task on </h2>
        <h2>Quality Assessment for Text Simplification (QATS)</h2>  
        <h3 style="color: #A7A7A7">28th May 2016</h3>
        <h3 style="color: #A7A7A7">Portorož, Slovenia</h3>
        
      </div>
    </header>


home 
# <A HREF="dates.html">important dates</A>
# <A HREF="shared.html">shared task</A>  
# <A HREF="submission.html">submission</A>
# <A HREF="proceedings.html">proceedings</A>
</center>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>

<p>This workshop aims to bring together researchers interested in all aspects of evaluation of automatic text simplification (ATS) systems, including automatic, human, and user-focused evaluation. It addresses the current problem in the text simplification community that there are no common standards and evaluation methodologies which would enable fair comparison of different ATS systems.</p>

<p>Given the close relatedness of the problem of automatic evaluation of ATS system to well-studied problems of automatic evaluation and quality estimation in machine translation (MT), the workshop also features a shared task on automatic evaluation (quality assessment) of ATS systems. We hope that this shared task will bring together the researches from MT and TS communities. We will provide training and test data-sets for English (obtained by using three different ATS approaches) and several baselines.</p>

<p>We solicit papers describing resources, models and techniques for evaluation of text simplification systems, proposing automatic evaluation metrics or discussing problems in user-focused evaluation, and the papers describing systems which participated in the shared task.</p>

<p>The workshop will feature an invited speaker, oral and poster presentations, and a closing panel discussion.</p>

<h2>
<a id="motivation-and-topics-of-interest" class="anchor" href="#motivation-and-topics-of-interest" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation and topics of interest</h2>

<p>In recent years, there has been an increasing interest in automatic text simplification (ATS) and text adaptation to various target populations. However, studies concerning evaluation of ATS systems are still very scarce and there are no methods proposed for directly comparing performances of different systems. Previous studies (Štajner et al. 2013) showed that machine translation automatic evaluation metrics such as BLEU, METEOR and TINE provide a good baseline for this task. </p>

<p>By organising a shared task, we wish to bring together researchers working on automatic evaluation and quality estimation of machine translation output, to participate and try to adapt their metrics to this closely related task. This would provide an opportunity to establish some metrics for automatic evaluation of ATS systems and enable their direct comparison in terms of the quality of the generated output, as well as less time consuming assessment of each ATS system. </p>

<p>We solicit papers on all aspects of evaluation of text simplification systems (automatic, human, and user-focused) hoping to provide an opportunity for researchers to exchange their experiences and discuss common problems in all aspects of ATS evaluation.  </p>

<p>Topics of interest include, but are not limited to:</p>

<ul>
<li>Resources for evaluation of text simplification systems and evaluation schemes</li>
<li>Automatic evaluation metrics and complexity metrics</li>
<li>Quality assessment for text simplification</li>
<li>Problems in human evaluation of text simplification systems</li>
<li>Problems in user-focused evaluation of text simplification systems</li>
<li>Similarities and dissimilarities of automatic evaluation of text simplification systems with automatic evaluation of machine translation and/or summarisation systems</li>
<li>NLP tools for enhancing evaluation of text simplification systems</li>
<li>User studies</li>
</ul>

<p> The participants in the shared task are invited to submit a short paper (4 to 6 pages) describing their evaluation method(s). 
You are not required to submit a paper if you do not want to. If you don't, we ask that you give an appropriate reference describing your method which we can cite in the overview paper.</p>

<h2>
<a id="submission-information" class="anchor" href="#submission-information" aria-hidden="true"><span class="octicon octicon-link"></span></a>Submission information</h2>

<p>We encourage contributions in the form of full papers (up to 8 pages long + 2 pages references) and short papers (up to 4 pages long + 2 pages references). 
Papers should follow the LREC format which is available at the LREC 2016 Web Site (<a href="http://lrec2016.lrec-conf.org/en/">http://lrec2016.lrec-conf.org/en/</a>).
The submission process has to be done online using the START conference system: <a href="https://www.softconf.com/lrec2016/QATS/">https://www.softconf.com/lrec2016/QATS/</a></p>

<p>All accepted papers will be presented orally or as a poster and published in the workshop proceedings.</p>

<h2>
<a id="identify-describe-and-share-your-lrs" class="anchor" href="#identify-describe-and-share-your-lrs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Identify, describe and share your LRs!</h2>

<p>Describing your LRs in the LRE Map is now a normal practice in the submission procedure of LREC (introduced in 2010 and adopted by other conferences).</p>

<p>To continue the efforts initiated at LREC 2014 about “Sharing LRs” (data, tools, web-services, etc.), authors will have the possibility, when submitting a paper, to upload LRs in a special LREC repository. This effort of sharing LRs, linked to the LRE Map for their description, may become a new “regular” feature for conferences in our field, thus contributing to creating a common repository where everyone can deposit and share data.</p>

<p>As scientific work requires accurate citations of referenced work so as to allow the community to understand the whole context and also replicate the experiments conducted by other researchers, LREC 2016 endorses the need to uniquely identify LRs through the use of the International Standard Language Resource Number (ISLRN, <a href="http://www.islrn.org">www.islrn.org</a>), a Persistent Unique Identifier to be assigned to each Language Resource. The assignment of ISLRNs to LRs cited in LREC papers will be offered at submission time.</p>

<h2>
<a id="important-dates" class="anchor" href="#important-dates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Important dates</h2>

<p>Shared task description and release of training data: 9th December 2015<br>
Shared task release of test set: 20th January 2016<br>
Submission of shared task results: 29th January 2016 <font color=red> Deadline extended until 3rd February 2016.</font><br>
Paper submission deadline: 7th February 2016<br>
Notification of acceptance: 8th March 2016<br>
Camera-ready version: 25th March 2016<br>
Workshop: 28th May 2016  </p>

<h2>
<a id="organisation-committee" class="anchor" href="#organisation-committee" aria-hidden="true"><span class="octicon octicon-link"></span></a>Organisation committee</h2>

<p>Sanja Štajner (University of Mannheim, Germany)<br>
Maja Popović (Humboldt University of Berlin, Germany)<br>
Horacio Saggion (Universitat Pompeu Fabra, Spain)<br>
Lucia Specia (University of Sheffield, UK)<br>
Mark Fishel (University of Tartu, Estonia)  </p>

<h2>
<a id="program-committee" class="anchor" href="#program-committee" aria-hidden="true"><span class="octicon octicon-link"></span></a>Program committee</h2>

<p>Sandra Aluisio (University of São Paolo)<br>
Eleftherios Avramidis (DFKI Berlin)<br>
Susana Bautista (Federal University of Rio Grande do Sul)<br>
Stefan Bott (University of Stuttgart)<br>
Richard Evans (University of Wolverhampton)<br>
Mark Fishel (University of Tartu)<br>
Sujay Kumar Jahuar (Carnegie Mellon University)<br>
David Kauchak (Pomona College)<br>
Ruslan Mitkov (University of Wolverhampton)<br>
Gustavo Paetzold (University of Sheffield)<br>
Maja Popović (Humboldt University of Berlin)<br>
Miguel Rios (University of Leeds)<br>
Horacio Saggion (Universidad Pompeu Fabra)<br>
Carolina Scarton (University of Sheffield)<br>
Matthew Shardlow (University of Manchester)<br>
Advaith Siddharthan (University of Aberdeen)<br>
Lucia Specia  (University of Wolverhampton)<br>
Miloš Stanojević (University of Amsterdam)<br>
Sanja Štajner (University of Mannheim)<br>
Irina Temnikova (Qatar Computing Research Institute)<br>
Sowmya Vajjala (Iowa State University)<br>
Victoria Yaneva (University of Wolverhampton)  </p>

<h2>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contact</h2>

<p>For further information please contact us at:<br>
<a href="mailto:stajner.sanja@gmail.com">stajner.sanja@gmail.com</a> or <a href="mailto:maja.popovic@hu-berlin.de">maja.popovic@hu-berlin.de</a></p>
        </section>

 
      </div>
    </div>

  
  </body>
</html>
